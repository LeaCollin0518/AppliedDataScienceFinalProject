---
title: " final project"
author: "Mayur"
date: 
output:
  prettydoc::html_pretty:
    theme: tactile
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r libraries, echo=FALSE,include=FALSE}
library(dplyr) 
library(plyr)
library(ggplot2)
library(tidyverse)
library(data.table)
library(DT)         
library(lubridate)    #dealing with dates
library(randomForest) #random forest algo
library(DataExplorer) #plot missing values
library(caret) #splitting dataset
library(Hmisc) #function cut2
library(dlookr) #find skewness
library(glmnet) #lasso
library(e1071) #skewness func
library(bit64)
library(corrplot)
library(rpart)
library(rpart.plot)
```


```{r read data}
all.data<-fread('NYCRealEstateFullData.csv',verbose=FALSE)
```

```{r constants}

old.borough.name <- "BOROUGH"
neighborhood.name <- "NEIGHBORHOOD"
building.class.name <- "BUILDING CLASS CATEGORY"
tax.class.name <- "TAX CLASS AT PRESENT"
block.name <- "BLOCK"
lot.name <- "LOT"
easement.name <- "EASE-MENT"
building.class.present.name <- "BUILDING CLASS AT PRESENT"
address.name <- "ADDRESS"
apartment.number.name <- "APARTMENT NUMBER"
zip.name <- "ZIP CODE"
residential.name <- "RESIDENTIAL UNITS"
commercial.name <- "COMMERCIAL UNITS"
total.units.name <- "TOTAL UNITS"
land.square.feet.name <- "LAND SQUARE FEET"
gross.square.feet.name <- "GROSS SQUARE FEET"
year.built.name <- "YEAR BUILT"
tax.class.sale.name <- "TAX CLASS AT TIME OF SALE"
building.class.sale.name <- "BUILDING CLASS AT TIME OF SALE"
sale.price.name <- "SALE PRICE"
sale.date.name <- "SALE DATE"
sale.year<-"SALE YEAR"
sale.month<-"SALE MONTH"
```

```{r functions}
## Data size function
set.size<-function(n,dat){
  the.rows<-sample(x=1:nrow(dat),size=n,replace = FALSE)
  return(dat[the.rows,])
}

linear.regression.summary <- function(lm.mod, digits = 3, alpha = 0.05) {
lm.coefs <- as.data.table(summary(lm.mod)$coefficients,
keep.rownames = TRUE)
setnames(x = lm.coefs, old = "rn", new = "Variable")
z <- qnorm(p = 1 - alpha/2, mean = 0, sd = 1)
lm.coefs[, Coef.Lower.95 := Estimate - z * `Std. Error`]
lm.coefs[, Coef.Upper.95 := Estimate + z * `Std. Error`]
return(lm.coefs[])
}

```

# Occurences by Borough 
## Investigation
## We saw the occurrences of properties based on the borough type. 

```{r count_borough,echo=FALSE}
borough.wise.occurences<-all.data[,.N,`Fixed Borough`]
borough.wise.occurences<-setorderv(borough.wise.occurences,order=-1,cols="N")
datatable(borough.wise.occurences)
```

## Result

## The results show us that Brooklyn and Manhattan have more listed properties (twice) than other boroughs and must be taken into consideration for further analysis. 


# Analyzing Sale Prices with respect to Month of Sale

## Investigation
## Investigating the effect of sale month on the sale prices of the properties.  

```{r fix sale.date}
all.data$`SALE DATE`<-mdy(all.data$`SALE DATE`)    #conversion from factor to date format
all.data$`SALE YEAR`<-year(all.data$`SALE DATE`)   #extracting year from sale date
all.data$`SALE MONTH`<-month(all.data$`SALE DATE`) #extracting months from sale date 
occurences<-all.data[, .N, eval(sale.month)] #evenly distributed

month.wise.sale<-all.data[,.("Mean Sale"=mean(`SALE PRICE`)),sale.month] 
monthly.sales<-merge(month.wise.sale,occurences,by = sale.month)
monthly.sales<-setorderv(monthly.sales,order=-1,cols="Mean Sale")
datatable(monthly.sales)
```

## Result
## On analyzing monthly sales of properties in NYC, it is seen that December, June and January have the highest average sale price (going higher than 1 million dollars) whereas properties sold in March, September remain less expensive



# Checking Data Quality

## Challenge 1 (Missing Data)

## One of the most challenging part about the dataset was to treat the missing and the unexpected values. First we dealt with the missing data specifically. The variables which were not relevant to the analysis and contained missing data were not taken into consideration for further analysis. 

```{r missing data exploration}
plot_missing(all.data) #100% missing values for Ease.ment, 73% missing values in apartment number (can be removed from analysis)
```


# Unexpected Values in Dataset (Challenge #2)

## Challenge 2 (unexpected values)


## After inspecting the data more carefully, we found certain values which are not missing, but still hamper with the analysis. For example, 30% of the data had a sale price of 0 which does not make sense this could be cause due to a quit claim deed [reference](https://info.courthousedirect.com/blog/bid/309758/how-does-a-quitclaim-deed-affect-your-mortgage ) or according to the data source, a change in owenrship of the property. This was further complicated by the fact that the same property could be present in the dataset with different sale prices (including 0), which may not give us a fair idea about the pricing trends. For these reasons, we decided to simply use properties with a positive sale price. For some analysis and visualizations, we even subsetted the data further because there were also sale prices listed as '1' or '10', which is illogical for the same reasons as a sale price of 0. 

## Additionally, the year built variable has 0 values which do not make logical sense and to overcome that we decided to bin the year built values in brackets to give more depth to our analysis. 

## Data Cleaning

```{r cleansing data}
setDT(all.data)
all.data.tax.class.na<-all.data[is.na(`TAX CLASS AT PRESENT`),.SD] #20640 na values
#option 1: to remove these 20640 values from dataset as most of other variables like land sqr feet are also 0 and also 20640 missing values in building class at present also correspond to these values

#cleaning dataset
all.data.clean<-all.data[!is.na(all.data$`TAX CLASS AT PRESENT`),.SD]
all.data.clean<-all.data.clean[!is.na(`BUILDING CLASS CATEGORY`),.SD]
all.data.clean$`YEAR BUILT`[all.data.clean$`YEAR BUILT`== 0] <- NA

#making brackets for year built(grouping prop based on the year they were built in)
cuts.year <- c(1900,1925,1950,1975,2000)
all.data.clean[, eval(year.built.name) := cut2(x = get(year.built.name), cuts = cuts.year)]
#removing 0 values from sale price
all.data.clean$`SALE PRICE`[all.data.clean$`SALE PRICE`==0]<-NA
all.data.clean$`LAND SQUARE FEET`[all.data.clean$`LAND SQUARE FEET`==0]<-NA
all.data.clean$`GROSS SQUARE FEET`[all.data.clean$`GROSS SQUARE FEET`==0]<-NA

plot_missing(all.data.clean)
find_skewness(all.data.clean) #18 i.e sale price is skewed
#transform sale price to reduce skewness in data
#all.data.clean$SALE.PRICE=transform(all.data.clean$SALE.PRICE)
#all.data.clean$LAND.SQUARE.FEET=transform(all.data.clean$LAND.SQUARE.FEET)
#all.data.clean$GROSS.SQUARE.FEET=transform(all.data.clean$GROSS.SQUARE.FEET)

all.data.clean<-all.data.clean[!is.na(all.data.clean$`YEAR BUILT`),.SD]
all.data.clean<-all.data.clean[!is.na(all.data.clean$`SALE PRICE`),.SD]
all.data.clean<-all.data.clean[!is.na(all.data.clean$`LAND SQUARE FEET`),.SD]
all.data.clean<-all.data.clean[!is.na(all.data.clean$`GROSS SQUARE FEET`),.SD]
all.data.clean<-select(all.data.clean,c(-7,-10))
plot_missing(all.data.clean)
```


# Correlations

## Investigations

## We investigated the relation between sale price and certain relevant variables by finding the correlations between them. 

```{r correlations}
m<-cor(all.data.clean$`GROSS SQUARE FEET`,all.data.clean$`SALE PRICE`) 
n<-cor(all.data.clean$`LAND SQUARE FEET`,all.data.clean$`SALE PRICE`)  
o<-cor(all.data.clean$`TOTAL UNITS`,all.data.clean$`SALE PRICE`)

correlations<-all.data.clean[,.("Land Square Feet and Sale Price"=n,"Gross Square Feet and Sale Price"=m,"Total Units and Sale Price"=o)]
correlations<-round(correlations,3)
datatable(correlations)
```

## Result

- Gross Square Feet and Sale price are strongly correlated (0.63)
- However, Land square feet and Sale price show weak correlation (0.15)
- Total Units and Sale price also show a moderate correlation (0.474)


# Analyzing Neighborhoods in terms of their Sale Prices 

## Investigation

## Since we decided to look into variables which are assdociated with sale price, neighborhood is one of the variables which is relevant and needs to be analyzed before including it in our models. We start by examining the neighborhoods by their occurences and mean prices 

```{r neighborhood analysis}
neigh<-all.data.clean[,.N,NEIGHBORHOOD]
setorderv(neigh,col="N",order=-1)

neigh.50<-neigh[1:50,]

neigh.price<-all.data.clean[,.('mean price'=mean(`SALE PRICE`)),NEIGHBORHOOD]
setorderv(neigh.price,cols="mean price",order=-1)

neigh.analysis<-merge(neigh,neigh.price,by = neighborhood.name)
setorderv(neigh.analysis,cols="mean price",order=-1)
datatable(neigh.analysis)
```

## Results

## Results show us that Midtown CBD and KIPS Bay are neighborhoods with highest mean sale prices. However we also notice that apart from mean prices, the occurences of these properties are important to note since a few properties with higher average value can undermine the influence of high number of properties with slightly lesser mean price. So we took that into account for our analyis.


# Scaling Prices

## We checked for skewness of different variables in the dataset and found that sale price, which is the dependent variable in our machine learning models, is skewed. Therefore, we standardized the variable by scaling the prices. 

```{r scaling the prices}
all.data.clean$`SALE PRICE`=scale(all.data.clean$`SALE PRICE`)
```

# Prices are skewed and therefore are standardized by using scale function. 


# Machine Learning

## Investigation

## Linear Regression:

## Model 1:  Once we cleaned the data, we built a simple linear regression model based on the variables which seemed relevant (based on intuition). These variables include year built, borough, total units, land square feet, gross square feet, sale year and sale month. 

```{r model 1}
#splitting
set.seed(100)
a<-createDataPartition(all.data.clean$`SALE PRICE`,p = 0.70,groups =100,list=FALSE)
traina<-all.data.clean[a,]
testa<-all.data.clean[-a,]

#model
mod1<-lm(`SALE PRICE`~`Fixed Borough`+`LAND SQUARE FEET`+`GROSS SQUARE FEET`+`SALE YEAR`+`TOTAL UNITS`+`YEAR BUILT`+`SALE MONTH`,data=traina)
summary(mod1) #0.49
mod1.pred<-predict(mod1,newdata=testa)
mod1.rmse = sqrt(mean((mod1.pred-testa$`SALE PRICE`)^2)) #0.83

#library(car)
#vif(mod1)
#mod1.rmse #0.62

#linear.regression.summary(mod.lm)

#model.linear<-as.data.frame(linear.regression.summary(mod.lm))
#model.linear<-model.linear[,c(1,2,5)]
#model.linear.round<-round(model.linear[,-1],3)
#model.linear.final<-cbind(model.linear$Variable,model.linear.round)
#datatable(model.linear.final)
```

## Results

## We get a r-squared value of 0.45 based on the variables used in model 1. We notice that all these variables are statistically significant and hence have an impact in sale prices of different types of properties.


## Investigation

## Now intuition says that location plays a major role in creating a good predictive model when it comes to predicting property prices. Our major variables for location are Boroughs and Neighborhood. In our first model we included boroughs but not neighborhood, however neighborhood should give more precise information to our model and we tried to look at the improvement by just adding this variable in our next predictive model.

```{r model2,warning=FALSE}
mod2<-lm(`SALE PRICE`~`Fixed Borough`+`LAND SQUARE FEET`+`GROSS SQUARE FEET`+`SALE YEAR`+`TOTAL UNITS`+`YEAR BUILT`+`SALE MONTH`+NEIGHBORHOOD,data=traina)
summary(mod2) #0.50
#mod2.pred<-predict(mod2,newdata=testa)
#mod2.rmse = sqrt(mean((mod2.pred-testa$`SALE PRICE`)^2)) #0.79
```

## Results

## The results show us that in terms of r-squared value, the model has improved slightly from 0.45 to 0.50 which is a good sign and tells us that neighborhood has an improvement on the model. But the challenge is that many neighborhoods do not have enoiugh representation in the dataset and might hamper getting good predictions as we may overestimate the factor of location by including all the neighborhoods.

## Investigation

## To yield much better results from our predictive model, we decided to only include neighborhoods which are representative of the properties by taking their occurences and mean prices into account.We decided to go for top 50 representative neighborhoods (look at assumptions section)

```{r neigh 50,warning=FALSE}
neigh.analysis.50<-neigh.analysis[1:50]
selected.neighbors<-all.data.clean[get(neighborhood.name) %in% neigh.analysis.50$NEIGHBORHOOD]

#split
set.seed(100)
n<-createDataPartition(selected.neighbors$`SALE PRICE`,p = 0.70,groups =100,list=FALSE)
train.n<-selected.neighbors[n,]
test.n<-selected.neighbors[-n,]

#model
mod.n<-lm(`SALE PRICE`~NEIGHBORHOOD+`Fixed Borough`+`LAND SQUARE FEET`+`GROSS SQUARE FEET`+`SALE YEAR`+`TOTAL UNITS`+`YEAR BUILT`,data=train.n)
summary(mod.n)
predict.n<-predict(mod.n,newdata=test.n)
rmse.pred.copy<-sqrt(mean((predict.n-test.n$`SALE PRICE`)^2)) #2.13 (different test data than first 2 models)

```

## Results

## We see some interesting results with this approach, the R squared value goes up to 0.59 (improvement in r squared value by just using a subset of neighborhood levels. This shows us that neighborhood is important but it is more beneficial to filter the information as too much information can ultimately overfit the model. However, for this project we did not use Boosting methods which might take this complexity into account. Still we realize that a cleansed neighborhood column would work better to fit a model. 

## Another thing to notice is that since we are using scaled prices as our dependent variable, we get estimate values which are scaled and not in terms of actual sale prices.

# Limitations and Challenges

## One limitation of using scaled prices is that it is not interpretable since the dependent variable is scaled and the results of regression model are not in actual numbers (as in the original format of dataset). 

# Assumptions

## In our neighborhood analysis, we assume that top 50 neighborhoods for analysis are based on the mean prices and occurences of the properties in these neighborhoods. 

# Reference

## https://info.courthousedirect.com/blog/bid/309758/how-does-a-quitclaim-deed-affect-your-mortgage

