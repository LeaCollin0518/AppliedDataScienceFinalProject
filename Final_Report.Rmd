---
title: "A Deep Dive Into New York City Real Estate"
author: "Unique Team B: Mayur Bansal, Huaqiu Chen, Lea Collin, Gengyu Zhang"
date: "05/09/2019"
output:
  prettydoc::html_pretty:
  theme: cayman
highlight: github
---

Note: All of the code for this project can be found [here](https://github.com/LeaCollin0518/AppliedDataScienceFinalProject).

```{r load_libraries, include = FALSE}
library(data.table)
library(DT)
library(tidyverse)
library(extracat)
library(lubridate)
library(Hmisc)
library(choroplethr)
library(choroplethrZip)
library(DataExplorer) #plot missing values
library(caret) #splitting dataset
library(Hmisc) #function cut2
library(dlookr) #find skewness
library(glmnet) #lasso
library(e1071) #skewness func
library(bit64)
library(corrplot)
library(rpart)
library(rpart.plot)
```

```{r load_data, include = FALSE}
setwd("~/Columbia/AppliedDS/FinalProject/AppliedDataScienceFinalProject")
all.data <- fread("Data/NYCRealEstateFullData.csv")
```

```{r functions, include = FALSE}
avg.sale.price.by <- function(data, by.column.names){
  mean.price <- data[, .(`Avg. Price` = mean(get(sale.price.name), na.rm=TRUE)), by = by.column.names]
  return (mean.price)
}

graph.choropleth <- function(data, year){
  keep_cols = c(zip.name, sale.price.name)
  data(zip.regions)
  zip.prices <- data[get(sale.year.name) == year, ..keep_cols]
  zip.prices <- zip.prices[, mean(get(sale.price.name), na.rm = TRUE), by = zip.name]
  colnames(zip.prices) <- c("region", "value")
  zip.prices$value <- as.numeric(zip.prices$value)
  zip.prices$region <- as.character(zip.prices$region)
  zip.prices <- zip.prices[region %in% zip.regions$region,]
  zip.prices <- zip.prices[value > 0,]
  
  plot.title <- c(year, " Average Sale Price by Zip Code")
  plot.title <- paste(plot.title, collapse="")
  
  choro.graph <- zip_choropleth(zip.prices,
                 zip_zoom = zip.prices$region,
                 title       =  plot.title,
                 legend      = "Average Sale Price")
  return (choro.graph)
}


## Data size function
set.size<-function(n,dat){
  the.rows<-sample(x=1:nrow(dat),size=n,replace = FALSE)
  return(dat[the.rows,])
}

linear.regression.summary <- function(lm.mod, digits = 3, alpha = 0.05) {
  lm.coefs <- as.data.table(summary(lm.mod)$coefficients,
  keep.rownames = TRUE)
  setnames(x = lm.coefs, old = "rn", new = "Variable")
  z <- qnorm(p = 1 - alpha/2, mean = 0, sd = 1)
  lm.coefs[, Coef.Lower.95 := Estimate - z * `Std. Error`]
  lm.coefs[, Coef.Upper.95 := Estimate + z * `Std. Error`]
  return(lm.coefs[])
}
```

```{r constants, include = FALSE}
old.borough.name <- "BOROUGH"
borough.name <- "Fixed Borough"
neighborhood.name <- "NEIGHBORHOOD"
building.class.name <- "BUILDING CLASS CATEGORY"
tax.class.name <- "TAX CLASS AT PRESENT"
block.name <- "BLOCK"
lot.name <- "LOT"
easement.name <- "EASE-MENT"
building.class.present.name <- "BUILDING CLASS AT PRESENT"
address.name <- "ADDRESS"
apartment.number.name <- "APARTMENT NUMBER"
zip.name <- "ZIP CODE"
residential.name <- "RESIDENTIAL UNITS"
commercial.name <- "COMMERCIAL UNITS"
total.units.name <- "TOTAL UNITS"
land.square.feet.name <- "LAND SQUARE FEET"
gross.square.feet.name <- "GROSS SQUARE FEET"
year.built.name <- "YEAR BUILT"
tax.class.sale.name <- "TAX CLASS AT TIME OF SALE"
building.class.sale.name <- "BUILDING CLASS AT TIME OF SALE"
sale.price.name <- "SALE PRICE"
sale.date.name <- "SALE DATE"
sale.year.name <- "Sale Year"
log.price.name <- "Log Price"
sale.month<-"SALE MONTH"
sale.year<-"SALE YEAR"
building.class.first.letter <- "Building Class First Letter"
year.built.name <- "YEAR BUILT"
borough_name <- c("Manhattan", "Bronx", "Brooklyn", "Queens", "Staten Island")
residential.group.name <- "RESIDENTIAL UNITS GROUP"
commercial.group.name <- "COMMERCIAL UNITS GROUP"
total.units.group.name <- "TOTAL UNIT GROUP"
land.group.name <- "LAND SQUARE FEET GROUP"
gross.group.name <- "GROSS SQUARE FEET GROUP"
cuts.total <- c(0, 1, 2, 3, 5)
cuts.land <- c(0, 100, 1000, 2000, 3000)
all.data[, eval(total.units.group.name) := cut2(x = get(total.units.name), cuts = cuts.total)]
all.data[, eval(land.group.name) := cut2(x = get(land.square.feet.name), cuts = cuts.land)]
unique.total.units.group <- all.data[, unique(get(total.units.group.name))]
unique.land.group <- all.data[, unique(get(land.group.name))]
scaled.price.name <- "Scaled Price"
scaled.total.units <- "Scaled Total Units"
scaled.square.feet <- "Scaled Square Feet"
all.data[, eval(scaled.price.name) := scale(x = get(sale.price.name))]
all.data[, eval(scaled.total.units) := scale(x = get(total.units.name))]
all.data[, eval(scaled.square.feet) := scale(x = get(land.square.feet.name))]

# also taking the log for future graphing
all.data[, `Log Price` := log(get(sale.price.name))]
all.data[, `Sale Year` := year(get(sale.date.name))]
all.data[, `Building Class First Letter` := substr(get(building.class.sale.name), 1, 1)]
dat <- all.data[get(sale.price.name) > 50000]
```

```{r fix sale.date, include = FALSE}
all.data$`SALE DATE`<-ymd(all.data$`SALE DATE`)    #conversion from factor to date format
all.data$`SALE YEAR`<-year(all.data$`SALE DATE`)   #extracting year from sale date
all.data$`SALE MONTH`<-month(all.data$`SALE DATE`) #extracting months from sale date 
```

```{r cleaning_data, include = FALSE}
setDT(all.data)
all.data.tax.class.na<-all.data[is.na(`TAX CLASS AT PRESENT`),.SD] #20640 na values
#option 1: to remove these 20640 values from dataset as most of other variables like land sqr feet are also 0 and also 20640 missing values in building class at present also correspond to these values

#cleaning dataset
all.data.clean<-all.data[!is.na(all.data$`TAX CLASS AT PRESENT`),.SD]
all.data.clean<-all.data.clean[!is.na(`BUILDING CLASS CATEGORY`),.SD]
all.data.clean$`YEAR BUILT`[all.data.clean$`YEAR BUILT`== 0] <- NA

#making brackets for year built(grouping prop based on the year they were built in)
cuts.year <- c(1900,1925,1950,1975,2000)
all.data.clean[, eval(year.built.name) := cut2(x = get(year.built.name), cuts = cuts.year)]
#removing 0 values from sale price
all.data.clean$`SALE PRICE`[all.data.clean$`SALE PRICE`==0]<-NA
all.data.clean$`LAND SQUARE FEET`[all.data.clean$`LAND SQUARE FEET`==0]<-NA
all.data.clean$`GROSS SQUARE FEET`[all.data.clean$`GROSS SQUARE FEET`==0]<-NA

all.data.clean<-all.data.clean[!is.na(all.data.clean$`YEAR BUILT`),.SD]
all.data.clean<-all.data.clean[!is.na(all.data.clean$`SALE PRICE`),.SD]
all.data.clean<-all.data.clean[!is.na(all.data.clean$`LAND SQUARE FEET`),.SD]
all.data.clean<-all.data.clean[!is.na(all.data.clean$`GROSS SQUARE FEET`),.SD]
all.data.clean<-select(all.data.clean,c(-7,-10))
```

```{r scaling the prices, include = FALSE}
all.data.clean$`SALE PRICE`=scale(all.data.clean$`SALE PRICE`)
```

# Introduction

For our final project, we looked at New York City real estate data ranging from 2003 to 2017. The data files include information such as neighborhood, building type, number of units, among many other features. The overarching question of our project is: what factors are important in determining sale price in New York? Throughout this project, we performed extensive exploratory data analysis and built several machine learning models to address this question. However, our analysis was not only limited to this question. We also tried to detects facts and trends in the New York City real estate market through different visualization methods.

This problem is important because New York is notorious for having some of the most expensive real estate in the nation. We were interested in discovering what factors influence these high price tags. When it comes to real estate, it is always said “location, location, location”, we want to find out whether it is true, how it affects house price if it is true and if there are any other factors that can also affect house price. This research could be used by any interested buyer who wants to perhaps predict the best time to buy, what areas of New York to buy in, and maybe what properties would give the largest returns.

# Sources of Data

The data was obtained from the NYC Department of Finance. The link to the data can be found [here](https://www1.nyc.gov/site/finance/taxes/property-annualized-sales-update.page). After finding this dataset, we looked at some of the excel files linked on the page and they seemed straightforward to understand and what we hoped for in trying to answer our main question. As the data came from the official Department of Finance, it can be considered a (presumably) reliable resource. It contains data across 15 years with a number of variables so we felt it could represent the state of New York City real estate quite well. We thought there would be many patterns we could discover since the dataset covers 15 years, a time period where a lot of things can change and a time period that contains the financial crisis. Furthermore, the dataset has information at the borough and even neighborhood and zip code level so we felt that this would greatly help us answer our questions related to the importance of location. 

# Examination of the Data

The original dataset was broken up by borough. This means there were 5 files for each year (one file per borough) and 15 years so there were 75 separate excel files in total. Thankfully, the columns of each file lined up in obvious ways and we were able to merge all the files into a single .csv file for ease of analysis. The code used to merge all the data can be found [here](https://github.com/LeaCollin0518/AppliedDataScienceFinalProject/blob/master/DataCleaning.Rmd).

The first change we made to the data was re-encoding the borough variable. In the original set, the boroughs were simply numbers. Because we assumed borough would be such an important variable, we wanted to make sure it was the actual borough name. The code to re-encode the variable can be found [here](https://github.com/LeaCollin0518/AppliedDataScienceFinalProject/blob/master/DataCleaning.Rmd).

We added a variable to the data which is the first letter of the building class codes. The description of all the building class codes can be found [here](https://www1.nyc.gov/assets/finance/jump/hlpbldgcode.html). Each code consists of a letter and a number. There are many codes but it seems that the letter describes the general type of building. For example, building codes starting in the letter "A" are types of one-family homes. 

We also wanted to know how much data came from each borough so we have a table below that simply shows the count of the number of rows in the dataset coming from each borough. Somewhat surprisingly, the boroughs we have the most rows from are Queens and Brooklyn when we previously thought it would be Manhattan. This could be because Manhattan has so many large buildings that even though many units are sold, many of them are sold all together as one property. 

```{r count_borough, echo = FALSE}
borough.counts <- all.data[, .N, by = borough.name]
colnames(borough.counts) <- c("Borough", "# Rows")
setorderv(x = borough.counts, cols = "# Rows", order = -1)
datatable(borough.counts)
```

We checked the quality of the data by inspecting each variable and how it is related to sale price. We also read a few articles on real estate to gain familiarity with the industry as certain things which originally seem a bit illogical might become clearer once we get an idea about the nuances of this particular sector. 


### Challenge #1 - Missing Data

One of the most challenging aspects of the dataset was treating missing and unexpected values. Below, we look at how many missing values there are in each column and the patterns of missing values.  

```{r count_na, echo = FALSE}
colSums(is.na(all.data))
```

```{r inspect_na, fig.height = 15, fig.width = 15, echo = FALSE}
visna(all.data, sort = "b")
```

We see that the most common columns that have missing values are "Ease-ment" and "Apartment Number". Both of these have missing values in more than half of all rows. It is probably safe to get rid of these columns and ignore them both in our exploratory analysis and our machine learning model as they likely do not hold any valuable information with so many missing values. It may have been interesting to look at apartment number, to see how the floor of the apartment (ie floor in the building) affects the apartment's price. Otherwise the only other missing valued columns are "Tax Class at Present", "Building Class at Present", "Building Class Category", and "Address". Address has only 7 missing values in over 1.4 million rows and there are many other columns that indicate the location of these buildings. We see from the graph that Tax Class at Present is always missing whenever Building Class at Present is missing. According to the data source, Tax Class at Present is "based on the use of the property" and Building Class at Present is "used to describe a property’s constructive use", so these two variables are very related. These two variables have the next highest number of missing values, but still only have about 20,000 missing which is not much when compared to the number of rows. 

```{r fix_na, include = FALSE}
all.data$`EASE-MENT` <- NULL
all.data$`APARTMENT NUMBER` <- NULL
```

### Challenge 2 - Unexpected Values

After inspecting the data more carefully, we found certain values which are not missing, but still hamper with the analysis. For example, 30% of the data had a sale price of 0 which does not make sense. This could be caused due to a quit claim deed [reference](https://info.courthousedirect.com/blog/bid/309758/how-does-a-quitclaim-deed-affect-your-mortgage ) or according to the data source, a change in ownership of the property. This was further complicated by the fact that the same property could be present in the dataset with different sale prices (including 0), which may not give us a fair idea about the pricing trends. For these reasons, we decided to simply use properties with a positive sale price. For some analysis and visualizations, we even subsetted the data further because there were also sale prices listed as 1 or 10, which is illogical for the same reasons as a sale price of 0. 

Additionally, the year built variable has 0 values which do not make sense and to overcome that we decided to bin the year built values in brackets to give more depth to our analysis. 

### Challenge 3 - Scaling Sale Price

We checked for skewness of different variables in the dataset and found that sale price, which is the dependent variable in our machine learning models, is skewed. Therefore, we standardized the variable by scaling the prices. Below, we plot the density of all sale prices below `$100,000,000 (which eliminates the exceptionally high prices and about 1000 rows) and we notice that we can't really see the distribution of the data at all. 

```{r inspect_sale_price, echo = FALSE}
density.data <- all.data[get(sale.price.name) < 100000000]

sale.density <- ggplot(density.data, aes(x=as.numeric(`SALE PRICE`))) + 
  geom_density(color = "blue", fill = "#99CCFF") + xlab("Sale Price")

sale.density
```


# Investigation

To answer our questions about the data and the topic, we first got acquainted with what was going on by creating different visualizations to inspect each variable. Our graphs included anything from density curves to visualize continuous variables to bar plots for categorical variables. We also produced line graphs to easily see trends over time. A lot of these graphs were included in our interactive component so that any reader can inspect the data on their own. The interactive component is actually very important because since there are so many years of data, it is hard to create so many visualizations in a static report. The interactive component easily allows the user to discover trends in different years and different variables. All of these graphs can be found in the following **Results** section. 

To actually answer our overarching question about what goes into determining sale price, we created three different linear regression models with different input variables. With these models, we were able to get a sense of which variables were most important based on their p-values and coefficient values. We also evaluated our models using the R-squared value. 

# Results

### Exploratory Data Analysis

Before we jumped into building our machine learning model, we did some detailed exploratory data analysis. This analysis starts with looking at the overall trend of the average sale price. Below, we see a graph of this trend for each borough and the overall dataset. 

```{r avg_sale_price_borough, fig.width = 10, echo = FALSE}
all.data.sub <- all.data[get(sale.price.name) > 0, ]

mean.year.borough.price <- avg.sale.price.by(all.data.sub, c(sale.year.name, borough.name))
mean.year.price <- avg.sale.price.by(all.data.sub, c(sale.year.name))
mean.year.price$`Fixed Borough` <- "Overall"

all.year.price <- rbind(mean.year.borough.price, mean.year.price)

setorderv(x = all.year.price, cols = "Sale Year", order = 1)
all.year.price <- all.year.price %>% mutate(`Fixed Borough` = forcats::fct_reorder2(`Fixed Borough`, `Sale Year`, `Avg. Price`))

year.borough.price.plot <- ggplot(all.year.price, aes(`Sale Year`, as.integer(`Avg. Price`), color = `Fixed Borough`)) + 
                   geom_line(size = 1) + geom_point(aes(`Sale Year`,as.integer(`Avg. Price`))) +
                   xlab("Sale Year") + ylab("Avg. Price") + labs(color = "Borough") +
                   scale_x_continuous(breaks = scales::pretty_breaks(length(unique(all.year.price$`Sale Year`)))) +
                   scale_color_manual(values=c("#3399FF", "#000000", "#66CC00", "#FF6633", "#9966FF", "#0066CC")) +
                   ggtitle("NYC Avg. Real Estate Price by Borough and Year") + theme_gray(base_size = 14)

year.borough.price.plot
```

Not too surprisingly, Manhattan sales prices are the highest among the five boroughs. In recent years, we have seen Brooklyn becoming a more sought-after borough which has led to increases in higher real estate prices, as corroborated by this graph. We should note that since this dataset does not only include apartment buildings, that that is probably also the reason that we see such high real estate prices in Manhattan. Manhattan has many more large buildings than, say, Staten Island which is much more residential. What is also interesting to note is that the Manhattan line follows basically the same trend as the overall city trend because it has the highest prices and is dominating the trend because of it. We see in all five boroughs and the overall trend that there is a dip in prices from 2007 to 2009, very likely caused by the financial crisis. This drop is most noticeable in Manhattan. There is an overall upward trend in the data, likely due to inflation and New York City's economic growth over the last 15 years. 

Another time trend we can look at is the average sale price by the building class type. Because there are many different building classes and a lot of what we care about is more of the residential real estate, we subset the data for this graph to only the classes that are more residential. We include this plot in our interactive component so that the user can see trends for more building class types. Note: there are `r all.data[get(building.class.present.name) != get(building.class.sale.name), .N]` rows where the building class at present and the building class at time of sale are not the same. For the purposes of visualization and subsetting, we used the building class at time of sale since we care about the sale price of the property. 

```{r get_residential, indluce = FALSE}
dat <- all.data[get(sale.price.name) > 50000]
residential.codes <- c("A", "B", "C", "D", "RR", "R1", "R2", "R3", "R4", "R6", "R7", "R8", "R9")
residential.properties <- all.data[get(building.class.first.letter) %in% residential.codes,]
```

```{r avg_sale_price_code, fig.width = 10, echo = FALSE}
mean.year.code.price <- avg.sale.price.by(residential.properties, c(sale.year.name, building.class.first.letter))
setorderv(x = mean.year.code.price, cols = "Sale Year", order = 1)
mean.year.code.price <- mean.year.code.price %>% mutate(`Building Class First Letter` = forcats::fct_reorder2(`Building Class First Letter`, `Sale Year`, `Avg. Price`)) # for coloring

year.code.price.plot <- ggplot(mean.year.code.price, aes(`Sale Year`, as.integer(`Avg. Price`), color = `Building Class First Letter`)) + 
                   geom_line(size = 1) + geom_point(aes(`Sale Year`,as.integer(`Avg. Price`))) +
                   xlab("Sale Year") + ylab("Avg. Price") + labs(color = "Building Class") +
                   scale_x_continuous(breaks = scales::pretty_breaks(length(unique(mean.year.code.price$`Sale Year`)))) +
                   ggtitle("NYC Avg.Residential Real Estate Price by Class Code and Year") + theme_gray(base_size = 14)

year.code.price.plot
```

We see that the ranking by price remains pretty consistent throughout the 15 year range. **C** and **D** are consistently more expensive, with **D** being the most, and **B** and **A** buildings are fairly similar in average price across the years. All of the **D** buildings are elevator buildings which could explain why they are more expensive. Additionally, the **D** class buildings have "luxury apartments" which could also be bumping up the average price. What's also interesting is that **D** buildings have more volatile average prices, though this could be due to the fact that the graph is on their scale and so we can't see the volatility in the lower average prices. Again we see a big dip in 2009 after the financial crisis.

We now turn our attention to the number of sales in each borough over the years. 

```{r number_of_sales, fig.width = 10, echo = F}
year <- substr(all.data$`SALE DATE`, 1, 4)
all.data$`SALE YEAR` <- year
number_sale_year <- as.data.frame(all.data[, (.N), by = `SALE YEAR`])
for (i in 1:5){
  a <- as.data.frame(all.data[BOROUGH == i, (.N), by = `SALE YEAR`])
  number_sale_year <- rbind(number_sale_year, a)
}
number_sale_year$Region <- c(rep('Overall', 15), rep('Manhattan', 15), rep('Bronx', 15),
                              rep('Brooklyn', 15), rep('Queens', 15), rep('Staten Island', 15))
colnames(number_sale_year)[2] <- 'Number of Sale'

number_year <- ggplot(data = number_sale_year, aes(x = `SALE YEAR`, y = `Number of Sale`, color = Region, group = Region)) +
  geom_line(size = 1) + 
  xlab('Year') +
  ylab('Number of Sales') +scale_color_manual(values = c("#FF6633", "#66CC00", "#3399FF", "#000000", "#9966FF", "#0066CC")) +
  theme_gray(base_size = 14)
number_year
```

Overall, the number of sales in New York City dropped during the first 7 years of the observed time period and then slightly rose between 2009 and 2010. The likely reason for the huge drop is the subprime mortgage crisis that we have already observed in our graphs above. We see the number of sales drop from 125,000 in 2006 to below 75,000 in 2010. After 2010, the housing market starts to recover and rises to about 87,000 sales from 2010 to 2012. After 2012, it remains at this level, still far from the numbers in 2003. As for the trend in each borough, Staten Island and the Bronx always had the lowest sales numbers and Queens almost always had the highest. All 5 boroughs were affected by the housing crisis. Apart from Manhattan, all other boroughs had similar trends to the overall trend. Manhattan sales numbers fluctuated more during this time frame. Numbers were rising slightly from 2003 to 2006 whereas all other borough sales were decreasing. It was decreasing slightly from 2013 to 2017 while the other boroughs were increasing their numbers. The explanation for this requires further investigation, perhaps into data with population, economics and financial conditions in each borough. 

Next we look more closely at each month in all the data. Here we have the average sale price and number of data points in each month. 

```{r points_by_month, echo = FALSE}
occurences<-all.data[, .N, eval(sale.month)] #evenly distributed

month.wise.sale<-all.data[,.("Mean Sale"=mean(`SALE PRICE`)),sale.month] 
monthly.sales<-merge(month.wise.sale,occurences,by = sale.month)
monthly.sales<-setorderv(monthly.sales,order=-1,cols="Mean Sale")
datatable(monthly.sales)
```

We see that December, June and January have the highest average sale price (going higher than 1 million dollars) whereas properties sold in March and September remain less expensive. We also see that the number of properties sold remains fairly consistent across all months.

Now we look more into the building class codes. Here, we visualize the number of buildings of each code class in each borough. Because there are so many building codes and many had such a negligible amount sold in the data, we plotted only the rows that had more than 250 sold in over the 15 years. Note: we are showing the raw counts, not the proportions. 

```{r building_codes, fig.height = 10, fig.width = 20, echo = FALSE}
summary.building.codes <- all.data[, .N, by = c(building.class.first.letter, borough.name)]

summary.building.codes <- summary.building.codes[N > 250, ]

code.plot <- ggplot(summary.building.codes, aes(x = reorder(`Building Class First Letter`, -1*`N`), y = `N`, fill = `Fixed Borough`)) +
             geom_bar(stat = 'identity', position="dodge") + xlab("Building Class Code") + ylab("Number of Sales") + labs(fill = "Borough") +
             scale_fill_manual(values=c("#FF6633", "#66CC00", "#3399FF", "#9966FF", "#0066CC")) +
             theme_gray(base_size = 20)

code.plot
```

There are a few interesting things that stand out from this graph. For one, Manhattan is the only borough that has buldings of type **H** and **L**. Buildings with a class code starting with **H** are generally hotels, motels, or hostels and **L** buildings are lofts. Hotels being sold only in Manhattan isn't too surprising as we know that there are many hotels there and the other boroughs are generally more residential. This doesn't mean that there are no hotels in the other boroughs however, it simply shows that none were **sold** during this 15 year period of time. Buildings with an **R** class code seem to be some of the most popular, but they are also the most varied. According to our data source, these properties range from indoor parking and office spaces to condos in buildings with other residential units. Building codes of class **A** are all of the one-family properties and we see that Queens and Staten Island have the highest number of those (Manhattan barely has any). Brooklyn and Queens have almost the same number of **B** properties sold, which are all two-family properties. All of this once again reaffirms our idea that Manhattan is mainly large buildings and the smaller boroughs are more residential. Some of the lowest counts in building class codes are **I** and **W**. **I** codes are for buildings such as nursing homes and hospitals and **W** is for schools and university buildings. 

To further reaffirm the finding that Manhattan has many less one-family and residential properties than the other boroughs, we can also look at tax class.

```{r tax_class_bar, echo = FALSE, fig.height = 8}
text.sale.borough <- all.data[, .N, by = c(tax.class.sale.name, borough.name)]

ggplot(data = text.sale.borough, mapping = aes(x = as.factor(get(tax.class.sale.name)), y = N)) +
  geom_bar(stat = "identity", color = "blue", fill = "#99CCFF") +
  labs(x = "Tax Class at Time of sale", y = "Number of Cases in each Group") +
  facet_grid(`Fixed Borough` ~ . ) + theme_gray(base_size = 14)
```

The most noteworthy feature is that there are far less properties with tax class 1 in Manhattan and properties with tax class 2 in Staten Island. Conversely, there are obviously more properties with tax class 2 in Manhattan. Considering tax class 1 refers to those small residential properties and tax class 2 refers to those properties including cooperatives and condominiums, it is no surprise that the amount of these tax classes differs greatly from Manhattan and Staten Island.

We can see if there is any relationship now between tax class and sale price. 

```{r tax_class_box, echo = FALSE}
tax.box.plot <- ggplot(data = dat, aes(x = reorder(as.factor(get(tax.class.sale.name)), -1*`Log Price`, FUN=median), y = get(log.price.name))) +
  geom_boxplot(fill = "#99CCFF") +
  labs(x = "Tax class at Time of Sale", y = "Log Sale Price") + theme_gray(base_size = 15)

tax.box.plot
```

We plot the log of the sale price here since the scale of price is so large. We see that buildings in the 3 tax class have the highest median price. This tax class is described by the data source as: includes property with equipment owned by a gas, telephone or electric company.  We see that tax class 2 and 1 have prices concentrated over a relatively small range of values compared to tax class 3. 

We also looked at the relationship between price per square foot and borough. Because many data points did not have square footage listed or had nonsensical values for gross square feet (ie 1), we decided to only plot residential properties. 

```{r mean_median, echo = F, fig.width = 10}
sub <- residential.properties[residential.properties$`SALE PRICE` != 0 & residential.properties$`GROSS SQUARE FEET` != 0]
sub$`PRICE PER FEET` <- sub$`SALE PRICE`/sub$`GROSS SQUARE FEET`
median_price_feet_2 <- as.data.frame(sub[, .(price = median(x = `PRICE PER FEET`, na.rm = T)), keyby = BOROUGH])
mean_price_feet_2 <- as.data.frame(sub[, .(price = mean(x = `PRICE PER FEET`, na.rm = T)), keyby = BOROUGH])
price_feet_2 <- rbind(median_price_feet_2, mean_price_feet_2)
price_feet_2$BOROUGH <- rep(borough_name, 2)
price_feet_2$Type <- c(rep('median', 5), rep('mean', 5))
price_feet_2$price <- round(price_feet_2$price, 0)

price_borough <- ggplot(data = price_feet_2, mapping = aes(x = reorder(factor(BOROUGH), -1*price),y = price, fill = Type)) +
  geom_bar(stat = "identity", position = 'dodge') + 
  scale_fill_manual(values = c('steelblue', 'orange')) +
  xlab('Borough') +
  ylab('Price Per Square Foot') +
  geom_text(aes(label = price), vjust = 1.5, colour = "white", position = position_dodge(.9), size = 4) +
  theme_gray(base_size = 14)

price_borough
```

This plot shows trends that confirm our intuitions and what other graphs have already shown us: Manahattan is the most expensive borough. We looked at both the mean and median as the median is more robust to outliers but the patterns are generally the same for both, except the median price per square foot is lower in Brooklyn than in Queens even though the mean ranking is reversed. 

We can also look at how price per square foot has changed across the boroughs in time.

```{r square_foot_year_borough, echo = F, fig.width = 8}
year <- substr(sub$`SALE DATE`, 1, 4)
sub$`SALE YEAR` <- year
price_feet_2_time <- as.data.frame(sub[, .(price = median(x = `PRICE PER FEET`, na.rm = T)), keyby = `SALE YEAR`])
for (i in 1:5){
  a <- sub[BOROUGH == i, .(price = median(x = `PRICE PER FEET`, na.rm = T)), keyby = `SALE YEAR`]
  price_feet_2_time <- rbind(price_feet_2_time, a)
}
price_feet_2_time$Region <- c(rep('Overall', 15), rep('Manhattan', 15), rep('Bronx', 15),
                              rep('Brooklyn', 15), rep('Queens', 15), rep('Staten Island', 15))

price_year <- ggplot(data = price_feet_2_time, aes(x = `SALE YEAR`, y = price, color = Region, group = Region)) +
  geom_line(size = 1) + 
  xlab('Year') +
  ylab('Price per Square Foot') +
  scale_color_manual(values = c("#FF6633", "#66CC00", "#3399FF", "#000000", "#9966FF", "#0066CC")) +
  theme_gray(base_size = 12)

price_year
```

In general, the price per square foot in New York city was rising over the 15 years, from about \$250 to about \$400. The most expensive borough is always Manhattan. We see that in 2006, the price in Manhattan is much higher than the other 4 boroughs. The least expensive borough is always the Bronx. The effect of the financial crisis is not too noticeable in the "Overall" line. Even when we look at each borough, the effect is still only drastically seen in Manhattan. From 2008 to 2009 there is a sharp drop in the price per square foot in Manhattan. These drops could be because most of the customers for real estate in Manhattan are investors, bankers, business men or financial firms and they are also the ones who lost the most during the financail crisis. We also notice that the price per square foot between the boroughs is more similar in the first three years than in the later years of the data. By checking data provided by [Barach College](https://www.baruch.cuny.edu/nycdata/consumer_prices/cpi.htm), CPI in New York raised from 197.8 in 2003 to 268.5 in 2017, which may magnify the gap amongst housing prices in different boroughs. 

Now we zoom in a bit more and look at how zip code (and therefore neighborhood) affect sale price. We have a choropleth map for the overall dataset shown below. We only look at the data for 2017 for simplicity in this static report. In our interactive component, we have two choropleth maps side-by-side where the user can choose two different years to better see if trends have changed over the years in each neighborhood. In the application, the user can additionally choose which building classes to include. 

```{r choropleth_map, echo = FALSE}
graph.choropleth(all.data, 2017)
```

As we can see, the range of values for sale price is enormous. Some areas have buildings with an average price of $2.2 billion! We have already noted that the likely reason for this is that some properties are large office buildings or even airport terminals. The most expensive buildings that are likely office spaces can be found in Downtown or Midtown Manhattan which makes sense as this is where the corporate side of New York resides.  

Because we said we are more interested in residential properties, we created the same graph only including these properties, again only for 2017. 

```{r residential_choropleth, echo = FALSE}
graph.choropleth(residential.properties, 2017)
```

Now that we are only including residential properties, the sales prices are easier to compare between parts of New York. Not surprisingly, many of the most expensive properites are in Manhattan. We see that Staten Island and the Bronx are probably the most affordable. Brooklyn and Queens are also more affordable except the areas that are closest to Manhattan are pricier. Even within Manhattan we see trends that make sense. Midtown and Downtown Manhattan see much higher average prices than anything at or above Central Park. 

We can look more closely at neighborhood by examining their occurrences and mean prices. 

```{r neighborhood analysis, echo = FALSE}
neigh<-all.data.clean[,.N,NEIGHBORHOOD]
setorderv(neigh,col="N",order=-1)

neigh.50<-neigh[1:50,]

neigh.price<-all.data.clean[,.('mean price'=mean(`SALE PRICE`)),NEIGHBORHOOD]
setorderv(neigh.price,cols="mean price",order=-1)

neigh.analysis<-merge(neigh,neigh.price,by = neighborhood.name)
setorderv(neigh.analysis,cols="mean price",order=-1)
datatable(neigh.analysis)
```

This table shows us that Midtown CBD and KIPS Bay are neighborhoods with the highest mean sale prices. However, we also notice that apart from mean prices, the occurences of these properties are important to note since a few properties with higher average values can undermine the influence of high numbers of properties with slightly lesser mean price. We took this into account for our analyis.

Next we took a look at how the total number of units and the square footage affect price. Without loss of generality, we will mainly focus on total units and land square feet here (and in fact the `total units` is just the sum of the residential units and commercial).

```{r scatter_unit_square.feet_1, echo = FALSE, cache= TRUE}
ggplot(data = dat) +
  geom_point(mapping = aes(x = get(scaled.total.units), y = get(scaled.price.name))) +
  labs(title = "Scaled NYC Real Estate Price by Total Units", x = "Scaled Total Units", y = "Scaled Sale Price")
```

```{r land_scatter, echo = FALSE, cache = TRUE}
ggplot(data = dat) +
  geom_point(mapping = aes(x = get(scaled.square.feet), y = get(scaled.price.name))) +
  labs(title = "Scaled NYC Real Estate Price by Land Square feet", x = "Scaled Land Square Feet", y = "Scaled Sale Price")
```

From these graphs, we see that a few points are outliers. We also see that most of the data is concentrated in the bottom left. Because of this, we're subsetting the data so that total units is greater than 0 and less than 5000, the land square feet is greater than 0 and less than 50,000,000, and sale price is greater than 50,000 and less than 5,000,000.

```{r scatter_unit_square.feet_2, echo = FALSE, cache = TRUE}
sub.data.unit.feet <- all.data[get(total.units.name) > 0 & get(total.units.name) < 5000 & get(land.square.feet.name) > 0 & get(land.square.feet.name) < 50000000 & get(sale.price.name) > 50000 & get(sale.price.name) < 5000000, ]
ggplot(data = sub.data.unit.feet) +
  geom_point(mapping = aes(x = get(scaled.total.units), y = get(scaled.price.name))) +
  labs(title = "Scaled NYC Real Estate Price by Total Units", x = " Scaled Total Units", y = "Scaled Sale Price")
```

```{r scatter_scaled, echo = FALSE, cache = TRUE}
scatter_sub_scaled <- ggplot(data = sub.data.unit.feet) +
  geom_point(mapping = aes(x = get(scaled.square.feet), y = get(scaled.price.name))) +
  labs(title = "Scaled NYC Real Estate Price by Land Square feet", x = "Scaled Land Square Feet", y = "Scaled Sale Price")
scatter_sub_scaled
```

We now see a slight positive correlation between total units and price and land square footage and price, but it is very slight. 

We further investigated the relation between sale price and certain relevant variables by finding the correlations between them. 

```{r correlations}
m<-cor(all.data.clean$`GROSS SQUARE FEET`,all.data.clean$`SALE PRICE`) 
n<-cor(all.data.clean$`LAND SQUARE FEET`,all.data.clean$`SALE PRICE`)  
o<-cor(all.data.clean$`TOTAL UNITS`,all.data.clean$`SALE PRICE`)

correlations<-all.data.clean[,.("Land Square Feet and Sale Price"=n,"Gross Square Feet and Sale Price"=m,"Total Units and Sale Price"=o)]
correlations<-round(correlations,3)
datatable(correlations)
```

Gross square feet and total units are both fairly strongly correlated with sale price (0.475 and 0.436 respectively). However, land square feet and sale price show a fairly weak correlation (0.01). Land square feet is defined as: the land area of the property listed in square feet. Gross square feet is defined as: the total area of all the floors of a building as measured from the exterior surfaces of theoutside walls of the building, including the land area and space within any building or structure on the property. 

Now we turn our attention to the count of the different number of total units and land square footage. We binned these values since there were not many distinct values and so that we could use a bar plot as our visualization.

```{r barplot_unit_square.feet_num, echo = FALSE, warning = FALSE}
unit.Num <- all.data[, .N, by = total.units.group.name]
ggplot(data = unit.Num, mapping = aes(x = get(total.units.group.name), y = N)) +
  geom_bar(stat = "identity", color = "blue", fill = "#99CCFF") +
  labs(x = "Group of Total Units", y = "Number of Cases in each Group") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

``` {r barplot_land_square, echo = FALSE}
land.Num <- all.data[, .N, by = land.group.name]
ggplot(data = land.Num, mapping = aes(x = get(land.group.name), y = N)) +
  geom_bar(stat = "identity", color = "blue", fill = "#99CCFF") + 
  labs(x = "Group of Land Square Feet", y = "Number of Cases in each Group") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

One interesting thing here is that most of the properties have total units lower than 5; however the bar plot tells us that the difference in price is mainly reflected in the group of total units higher than 5. Another interesting finding is that there is an obvious gap between the first group and the third when we are considering land square footage. It may be because the data actually splits nicely into properties that are smaller and residential and properties that are more commercial. We thought it may be reasonable to split the data into two large groups (small properties vs. large) but then we see from that the scatter plot that land square footage is not so predictive. We hold out for our linear regression models for further analysis.

### Machine Learning

After we finished our exploratory data analysis, we were finally ready to fit some models.  

#### Model 1 

Once we cleaned the data, we built a simple linear regression model based on the variables which seemed relevant (based on intuition). These variables include year built, borough, total units, land square feet, gross square feet, sale year and sale month. 

```{r model 1, echo = FALSE}
#splitting
set.seed(100)
all.data.clean$`SALE PRICE` <- as.numeric(all.data.clean$`SALE PRICE`)
a<-createDataPartition(all.data.clean$`SALE PRICE`,p = 0.70,groups =100,list=FALSE)
traina<-all.data.clean[a,]
testa<-all.data.clean[-a,]

#model
mod1<-lm(`SALE PRICE`~`Fixed Borough`+`LAND SQUARE FEET`+`GROSS SQUARE FEET`+`SALE YEAR`+`TOTAL UNITS`+`YEAR BUILT`+`SALE MONTH`,data=traina)
summary(mod1) #0.49
mod1.pred<-predict(mod1,newdata=testa)
mod1.rmse = sqrt(mean((mod1.pred-testa$`SALE PRICE`)^2)) #0.83
```

W get an R-squared value of 0.4162 based on the variables used in model 1. We notice that most of these variables are statistically significant and hence have an impact in sale prices of different types of properties. The variables that are not significant are total units, year build [1900-1925) and sale month.

#### Model 2

Now intuition says that location plays a major role in creating a good model when it comes to predicting property prices. Our major variables for location are Boroughs and Neighborhood. In our first model we included boroughs but not neighborhood. We think neighborhood should give more precise information to our model and we tried to look at the improvement by just adding this variable in our next predictive model, we kept all variablesfrom Model 1. 

```{r model2,warning=FALSE, echo = FALSE}
mod2<-lm(`SALE PRICE`~`Fixed Borough`+`LAND SQUARE FEET`+`GROSS SQUARE FEET`+`SALE YEAR`+`TOTAL UNITS`+`YEAR BUILT`+`SALE MONTH`+NEIGHBORHOOD,data=traina)
summary(mod2) #0.50
```

The results show us that in terms of R-squared value, the model has improved from 0.41 to 0.50 which is a good sign and tells us that neighborhood has an improvement on the model. The challenge is that many neighborhoods do not have enough representation in the dataset and might hamper us from getting good predictions as we may overestimate the factor of location by including all the neighborhoods.

#### Model 3

To yield much better results from our predictive model, we decided to only include neighborhoods which are representative of the properties by taking their occurences and mean prices into account. We decided to go for the top 50 representative neighborhoods (explained in the assumptions section below).

```{r neigh 50,warning=FALSE, echo = FALSE}
neigh.analysis.50<-neigh.analysis[1:50]
selected.neighbors<-all.data.clean[get(neighborhood.name) %in% neigh.analysis.50$NEIGHBORHOOD]

#split
set.seed(100)
n<-createDataPartition(selected.neighbors$`SALE PRICE`,p = 0.70,groups =100,list=FALSE)
train.n<-selected.neighbors[n,]
test.n<-selected.neighbors[-n,]

#model
mod.n<-lm(`SALE PRICE`~NEIGHBORHOOD+`Fixed Borough`+`LAND SQUARE FEET`+`GROSS SQUARE FEET`+`SALE YEAR`+`TOTAL UNITS`+`YEAR BUILT`,data=train.n)
summary(mod.n)
predict.n<-predict(mod.n,newdata=test.n)
rmse.pred.copy<-sqrt(mean((predict.n-test.n$`SALE PRICE`)^2)) #2.13 (different test data than first 2 models)

```

We see some interesting results with this approach: the R-squared value goes up to 0.59. This shows us that neighborhood is important but it is more beneficial to filter the information as too much information can ultimately overfit the model. However, for this project we did not use Boosting methods which might take this complexity into account. Still, we realize that a cleansed neighborhood column works better for this linear regression model. 

Another thing to notice is that since we are using scaled prices as our dependent variable, we get estimate values which are scaled and not in terms of actual sale prices. This is the likely reason why so many estimate values are small but still significant. 

# Interpretation

As one of the major finanical centers in the world, the real estate market in New York City is closely related to the financial market. The results show that, overall, New York real estate market suffered from the subprime mortagage crisis between 2007 and 2010. There was a significant drop in both housing prices and sales numbers. Until now, the market has not seen the number of sales that it saw in 2003. 

As for the condition in each borough, Manhattan is the most expensive borough and Brooklyn is the second. House prices in Queens and Staten Island are on similar levels and these two boroughs have the lowest average prices in New York. Furthermore, areas that are closer to Manhattan in other boroughs tend to have higher prices. 

From the regression model, we can see that the neighborhood variable is very significant, which confirms the point of "Location! Location! Location!". Also, the newer the building is, the higher the house price can be. 

Based on these results, buyers in the New York City real estate market can hopefully make more informed decisions. We already had a suspicion that location played a major role in determining sale price and we have now confirmed this concretely while also finding out that the year built is a major predictor. Though these results may not be groundbreaking, they are still informative. Buyers can now better weigh their options. If a buyer is looking for big office buildings or luxury apartments, they know that Manhattan is probably their best bet. If they're looking for the largest payoff in terms of price per square footage, then the Bronx is worth considering. Someone who wants something residential while still being close to the city may consider some neighborhoods in Queens or Brooklyn. These models may not change how the New York City real estate market is run, but it may change the way people look at it and handle it. 

# Assumptions

We made an assumption that our original data contained the main factors that affect New York city real estate price. In fact, from our own daily experience, most of the important factors when considering property prices like geographical location, purchase time, and housing type are all included in our data. In our models, the significance of these and their impact on price can also partially prove the rationality of this assumption.

# Limitations and Uncertainties

First, though we assumed our dataset was mostly "clean", we did have some records that were obviously not correct. Some housing prices were 1 or other small numbers that don't make sense as a sale price. What we did wehn facing these problems involving house pirce is subset the data to include prices larger than \$500,000. This is somewhat of an arbitrary selection but we felt this price was high enough to eliminate nonsensical values. This could have gotten rid of "clean" records and still kept around "dirty" records. Additionally, the land square feet and gross square feet suffered from similar issues as sale price so we had to subset the data as best we could.  

Second, we are not certain that this dataset contains every sale record. The data may have been collected intensively from some specific neighborhood while other neighborhoods have a lot of missing information. This can lead to biased results. A more accurate analysis can be done in the future by resampling with equal size from each neighborhood. 

Finally, when dealing with a huge dataset with many subgroups, Simpson's Paradox can happen easily. For instance, when we originally looked at the mean price per square foot in the whole dataset, the prices in Brooklyn were higher than that in Manhattan. However, if we looked at the price per foot just in the residential properties, the result is the opposite and the prices in Manhattan are higher than in Brooklyn. To avoid this paradox from happening, we should be clear about our questions in the first place and then select the correct subgroup in future analysis.

# Areas of Future Investigation

All the analysis was done based on the real estate data in New York city, however, as the finance center in the world, the real estate market in New York must be closely related to the finance market and economics conditions. Some economic factors such as GDP, CPI, population density should be added in the data in future investigations.

Location is not only about neighborhood and borough, it also includes distance to subway stations, restaurants, whole food stores and so on. A high quality neighborhood with high quality of life can obviously be more expensive than the neighborhood without any living facilities. However, due to the data limitation, investigation from a quality of life aspect cannot be done. 

Finally, all of our analysis is based on the data from the five boroughs in New York City. However, though Jersey City and Long Island are not New York administratively, they can be included in the Greater New York area geographically. From personal observation, we know that a great amount of people live in these two areas and work in the city. There may be more interesting results to uncover if the data from these 2 areas were included in future investigation. 


# References

**Title, Author, Link**

CPI Annual Averages, U.S. Department of Labor - Bureau of Labor Statistics, https://www.baruch.cuny.edu/nycdata/consumer_prices/cpi.htm

"How Does a Quitclaim Deed Affect Your Mortgage?", CourthouseDirect.com Team, https://info.courthousedirect.com/blog/bid/309758/how-does-a-quitclaim-deed-affect-your-mortgage
